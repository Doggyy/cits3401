\documentclass[11pt, a4paper]{article}

\usepackage{titling}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{appendix}

\newcommand{\subtitle}[1]{%
  \posttitle{%
    \par\end{center}
    \begin{center}\large#1\end{center}
    \vskip0.5em}%
}

\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}l@{}}#2\end{tabular}}
  
\setlength{\oddsidemargin}{0.5cm}
\setlength{\evensidemargin}{0.5cm}
\setlength{\topmargin}{-1.6cm}
\setlength{\leftmargin}{0.5cm}
\setlength{\rightmargin}{0.5cm}
\setlength{\textheight}{24.00cm} 
\setlength{\textwidth}{15.00cm}
\parindent 0pt
\parskip 5pt
\pagestyle{plain}
\usepackage[numbers]{natbib}
\usepackage{capt-of}
\usepackage{graphicx,url}

\begin{document}

\title{CITS3401 Data Exploration \& Mining Project 2}
\subtitle{Classifying Poker hands using Weka}
\author{Aleck Greenham 20362627 \\ Ash Tyndall 20915779}
\date{27th May 2013}

\maketitle

\section*{Introduction}

This document details the analysis of Na\"ive Bayesian, C4.5 and Backpropagation Neural Network Classifications methods for the classification of Poker hands (modelled as permutations of 5 playing cards) as one of the nine well-defined classes. Where the analysis requirements \cite{designdoc} were ambiguous or incomplete, reasonable assumptions were made and documented.

\section*{Classification Method Selection}

Classification methods were selected from those discussed in lectures based on how suitable each was for the classification necessary to correctly identify Poker hands. The in-program documentation of the classification methods (accessibly by right-clicking on the classifier name and selecting \textit{properties} from the resultant context menu) was also taken into consideration. 

\subsection*{Bayesian Classification}

The Na\"ive Bayesian Classifier was chosen for comparison with the other classifiers used. It is based on Bayes' Theorem:

\begin{align*}
	P(A|B) = \frac{P(B|A) \times P(A)}{P(B)}
\end{align*}

Where:

\begin{itemize}
	\item $P(A|B)$ is the probability that an element in a set B is also in set A
	\item $P(B|A)$ is the probability that an element in a set A is also in set B
	\item $P(A)$ is the probability that an element from the universal set is also in set A
	\item $P(B)$ is the probability that an element from the universal set is also in set B
\end{itemize}

When applied to classification, $P(A|B)$ becomes the probability a record in set B, belongs to a particular class, A. Na\"ive Bayesian Classification calculates the probability of a record belonging to each of the available classes, and places it in the most probable class.	

Record attributes are assumed to be independent to reduce the computational load in Bayesian classification. This is an appropriate assumption because each card has been split into two attributes - the value of the card and the suite - and therefore is almost entirely independent. The one exception is a hand with four of a kind; the fifth card's face value cannot be the same as the first four (because there are only 4 of each card denomination - one for each suite).

\subsection*{Decision Tree Classification}

Decision tree classification algorithms iteratively divide records based on attributes selected to increase information gain, until a given threshold is reached. Decision tree classification algorithms differ in the way they select attributes for division.

Attributes are required to be categorical values for use with decision tree classification. This is suitable for classifying poker hands as the attributes are categorical values (card face value and suite).

\textit{Information Gain} is a quantisation of the increase in information that can be achieved by sub-dividing a data set. It is therefore the objective of grouping the data to maximise information gain. Information gain for data set D, is defined as the difference in information or entropy before and after dividing the data:

\begin{align*}
	Gain(A) = Info(D) - Info_A(D)
\end{align*}

Where $Info(D)$ is the entropy of the entire data set:

\begin{align*}
	Info(D) = - \sum^{m}_{i=1}p_i\log_2(p_i)
\end{align*}

And $Info_A(D)$ is the information after a division A, into v different groups $(D_1,  D_2 \ldots, D_v)$:

\begin{align*}
	Info_A(D) = \sum^{v}_{j=1}\frac{|D_j|}{|D|} \times Info(D_j)
\end{align*}


C4.5 (or the J48 implementation in Weka) was selected from the available decision tree classification methods. It maximises a quantity called \textit{Gain Ratio}, a normalisation of Information Gain, to determine which attribute should be used to divide the data set to optimise information gain.

\begin{align*}
	Gain Ratio(A) = \frac{Gain(A)}{SplitInfo(A)}
\end{align*}

Where SplitInfo is defined as:

\begin{align*}
	SplitInfo_A(D) = - \sum^{v}_{j=1}\frac{|D_j|}{|D|} \times \log_2(\frac{|D_j|}{|D|})
\end{align*}

This iterative process of evaluation and division is stopped when all attributes have been used, each node belongs to the same class, or a pre-defined threshold has been exceeded.

\subsection*{Neural Network Classification}

Backpropagation Neural Network Classifications (or the MultilayerPerceptron implementation in Weka) work by assigning attribute values weightings, which are continually refined by examining the error rates of recent classifications until a predefined threshold is reached. The weightings are adjusted to minimise the mean squared error between the neural network's predictions and the correct classifications.

There are 3 layers of the network: the input, hidden and output layers. Attributes are first fed into the input layer, allocated weightings and then fed into the hidden layer(s). The output values of the hidden layer(s) are are weighted and passed to the output layer. The weightings are calibrated backwards: starting with the output layer and moving through the model towards the input layer.

\section*{Implementation}

\subsection*{Data Restructing}

Weka best supports its own \texttt{ARFF} format and so it was necessary to convert the CSV training \cite{trainingdata} and test \cite{testdata} data. The tool linked to in the design document \cite{arffconv} timed out and produced error 500 when attempting to convert the large testing set. A substitute Python script was written, \texttt{csv\_convertor.py}.

The documentation for the data attributes \cite{expattr} states the ``suit of card'' and ``class of hand'' attributes are ``ordinal'', yet the training data ARFF labelled all of its attributes as ``numeric''. To better reflect the nature of the data and to provide improved classifcation results, the ARFF file was modified to define the card value attributes (C1 through C5) to be Weka's ``nominal specification'' (Weka's form of an enumeration). 

The Python script mapped the suite attribute numbers to representative letter combinations to assist in the interpretation of the enumeration.

\subsection*{Creating Smaller Training Sets}

The Python script \texttt{training\_ratio\_splitter.py} creates smaller training sets while still maintaining the ratios of each type of Poker hand present in the larger training set. The program counts the number of the lowest probable hand class, Royal Flushes,  present in the larger training set and divides the full training set into smaller sets containing one Royal Flush each.

The ratio of classes is then calculated against Royal Flushes, and a segment of each class equivalent to that ratio is placed in each file. Because the training data set does not divide perfectly into integers, a small amount of card plays are discarded (36 of 25K). 

\subsection*{Splitting Testing Set}

\texttt{test\_splitter.py} randomly splits the test data into a configurable amount of sets with a configurable amount of elements. Ten sets of 5000 elements were generated in this instance. To prevent any result ordering bias, the entire test set is randomised in memory and then exported into the specified number of elements in their own file.

\section*{Experiment 1}

\subsection*{Aim}

The aim of the first experiment was to evaluate how effective each of the 3 classifiers are a categorising poker hands, as they formatted in the original training and test data sets. Each classifiers' classification error rate was calculated in order to compare the efficacy of the different methods.

\subsection*{Methodology}

The Na\"iveBayes, MultilayerPerceptron and the J48 algorithms were each trained using the complete training set \texttt{poker\_hand/training} (approximately 25,000 records) and tested using 10-fold cross-validation.

\subsection*{Results}

\subsubsection*{Na\"ive Bayes}

Using the 10-fold cross-validation, the na\"ive bayes method achieved a correct classification rate of 49.07\% and a root mean squared error of 0.24. The results show the method classified all hands as either being a hand worth nothing (0), or 1 pair (1). 

The strong favouring for the lower valued hands can be explained by the large difference in the frequency in which hands worth nothing and single pairs occur (49.95\% and 42.34\% respectively), and the much rarer hands such as Straight Flush (SF) or Royal Flush (RF) (0.02\% and 0.02\% respectively). The Na\"ive Bayes method calculates the the most probable class for a hand as a function of the likelihood of each card being part of a hand that is in that class. The lower valued hands occur most often and are much more likely and thus all cards are more likely to belong to one of the many hands that belong to these lower classes. It does not allow for the improbable occasions where these cards belong to a more valuable hand.

\begin{verbatim}
=== Detailed Accuracy By Class ===

               TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area  Class
                 0.921     0.929      0.497     0.921     0.646      0.503    0
                 0.072     0.077      0.408     0.072     0.122      0.497    1
                 0         0          0         0         0          0.508    2
                 0         0          0         0         0          0.474    3
                 0         0          0         0         0          0.667    S
                 0         0          0         0         0          0.482    F
                 0         0          0         0         0          0.56     FH
                 0         0          0         0         0          0.715    4
                 0         0          0         0         0          0.164    SF
                 0         0          0         0         0          0.381    RF
Weighted Avg.    0.491     0.497      0.421     0.491     0.374      0.501
\end{verbatim}

\begin{verbatim}
=== Confusion Matrix ===

     a     b     c     d     e     f     g     h     i     j   <-- classified as
 11510   983     0     0     0     0     0     0     0     0 |     a = 0
  9838   761     0     0     0     0     0     0     0     0 |     b = 1
  1129    77     0     0     0     0     0     0     0     0 |     c = 2
   480    33     0     0     0     0     0     0     0     0 |     d = 3
    87     6     0     0     0     0     0     0     0     0 |     e = S
    52     2     0     0     0     0     0     0     0     0 |     f = F
    33     3     0     0     0     0     0     0     0     0 |     g = FH
     6     0     0     0     0     0     0     0     0     0 |     h = 4
     4     1     0     0     0     0     0     0     0     0 |     i = SF
     4     1     0     0     0     0     0     0     0     0 |     j = RF
\end{verbatim}

\subsubsection*{Decision Tree}

The J48 algorithm showed a marked improvement above the Na\"ive Bayes on the 10-fold cross-validation with a correct classification rate of 57.54\% and a root mean squared error of 0.25. It was able to create rules that accommodate less likely hands such as 2 Pairs (2), 3 Pairs (3) and a Straight (S). However, it still failed to differentiate these from the more rare hands. This is not surprising for a classification method that still relies on probabilities (although they are ``specialised'' or weighted based on data subsets). 

It is interesting to note a large dip in precision for two pairs (2). From the confusion matrix it can be seen these are often misclassified as single pairs (1). This may be because it is much easier to determine the presence of a single pair than it is to confirm the presence of 2 pairs.

\begin{verbatim}
=== Detailed Accuracy By Class ===

               TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area  Class
                 0.808     0.555      0.592     0.808     0.684      0.656    0
                 0.403     0.242      0.55      0.403     0.465      0.588    1
                 0.016     0.005      0.129     0.016     0.028      0.657    2
                 0.004     0.001      0.065     0.004     0.007      0.645    3
                 0.011     0.001      0.05      0.011     0.018      0.569    S
                 0         0          0         0         0          0.534    F
                 0         0          0         0         0          0.623    FH
                 0         0          0         0         0          0.491    4
                 0         0          0         0         0          0.496    SF
                 0         0          0         0         0          0.498    RF
Weighted Avg.    0.575     0.38       0.537     0.575     0.54       0.626
\end{verbatim}

\begin{verbatim}
=== Confusion Matrix ===

     a     b     c     d     e     f     g     h     i     j   <-- classified as
 10100  2349    25     7     9     3     0     0     0     0 |     a = 0
  6207  4268    92    20     8     3     1     0     0     0 |     b = 1
   461   723    19     2     0     0     1     0     0     0 |     c = 2
   165   337     9     2     0     0     0     0     0     0 |     d = 3
    60    31     1     0     1     0     0     0     0     0 |     e = S
    45     9     0     0     0     0     0     0     0     0 |     f = F
     5    30     1     0     0     0     0     0     0     0 |     g = FH
     0     6     0     0     0     0     0     0     0     0 |     h = 4
     4     0     0     0     1     0     0     0     0     0 |     i = SF
     4     0     0     0     1     0     0     0     0     0 |     j = RF
\end{verbatim}

\subsection*{Neural Network}

Neural Networks achieved a surprising leap in correct classification rate with 93.43\% and a root mean squared error of 0.07 when run using 10-fold validation, compared to that of the other two classification algorithms. This is probably due to the fact that calibrated attribute weightings can be much more sensitive to rare hands than probabilities.

Of note is the large increase in precision for 3 of a Kind (3), which may be it can be satisfied in relatively few ways, and involves only 3 cards. The precision for hands than involve 4 or more cards is significantly lower. For example, 2 pairs (2) is often misclassified as a single pair (1). This is also consistent with hands involving a smaller number of cards having a higher precision classification rate. 

The execution time was considerably longer than that of the other two classification algorithms, yet in this case the huge increase in precision would justify the longer run time.
                 
\begin{verbatim}
=== Detailed Accuracy By Class ===

               TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area  Class
                 0.997     0.012      0.988     0.997     0.992      0.996    0
                 0.99      0.094      0.886     0.99      0.935      0.99     1
                 0.017     0.002      0.28      0.017     0.033      0.999    2
                 0.739     0.001      0.913     0.739     0.817      0.926    3
                 0.086     0.001      0.182     0.086     0.117      0.666    S
                 0.148     0          0.471     0.148     0.225      0.743    F
                 0         0          0         0         0          0.034    FH
                 0         0          0         0         0          0.324    4
                 0         0          0         0         0          0.334    SF
                 0         0          0         0         0          0.639    RF
Weighted Avg.    0.934     0.046      0.903     0.934     0.911      0.988
\end{verbatim}


\begin{verbatim}
=== Confusion Matrix ===

     a     b     c     d     e     f     g     h     i     j   <-- classified as
 12453     2     0     0    30     8     0     0     0     0 |     a = 0
    18 10497    54    27     2     1     0     0     0     0 |     b = 1
     0  1185    21     0     0     0     0     0     0     0 |     c = 2
     1   133     0   379     0     0     0     0     0     0 |     d = 3
    84     1     0     0     8     0     0     0     0     0 |     e = S
    43     1     0     0     2     8     0     0     0     0 |     f = F
     0    33     0     3     0     0     0     0     0     0 |     g = FH
     0     0     0     6     0     0     0     0     0     0 |     h = 4
     3     0     0     0     2     0     0     0     0     0 |     i = SF
     5     0     0     0     0     0     0     0     0     0 |     j = RF
\end{verbatim}

\begin{thebibliography}{9}

\bibitem{trainingdata}
	Poker Hand Training Data,	
	http://undergraduate.csse.uwa.edu.au/units/CITS3401/labs/poker-hand-training-true.data
	
\bibitem{testingdata}
	Poker Hand Test Data,	
	http://undergraduate.csse.uwa.edu.au/units/CITS3401/labs/poker-hand-testing.data
	
\bibitem{designdoc}
	CITS3401 Data Exploration and Mining - Project 2,
	http://undergraduate.csse.uwa.edu.au/units/CITS3401/labs/proj2-2013.html

\bibitem{arffconv}
	Online CSV to ARFF conversion tool,
	http://slavnik.fe.uni-lj.si/markot/csv2arff/csv2arff.php

\bibitem{expattr}
	Explanation of Poker Hand data attributes,
	http://undergraduate.csse.uwa.edu.au/units/CITS3401/labs/poker-hand.names

\bibitem{arffinfo}
	Attribute-Relation File Format,
	http://www.cs.waikato.ac.nz/ml/weka/arff.html

\end{thebibliography}

\bibliographystyle{IEEEtranN}

\end{document}